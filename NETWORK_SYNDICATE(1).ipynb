{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn1Bt2JfmZDU"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install scapy==2.5.0 cryptography==38.0.4 pandas==2.0.3 numpy==1.25.2 matplotlib==3.7.2 seaborn==0.12.2 -q\n",
        "!pip install fpdf==1.7.2 scikit-learn==1.3.0 nest_asyncio==1.5.8 dpkt==1.9.8 pyclamd==0.4.0 -q\n",
        "!pip install yara-python==4.5.1 requests==2.31.0 tensorflow==2.15.0 geocoder==1.38.1 qrcode==7.4.2 -q\n",
        "!pip install xgboost==2.0.3 lightgbm==4.3.0 tshark==0.7.2 pypsd==0.2.1 pyod==1.1.3 -q\n",
        "!apt-get update -q && apt-get install -y clamav tshark nikto -q && freshclam -q\n",
        "\n",
        "# All imports (moved before usage to avoid NameError)\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scapy  # Import scapy explicitly to access __version__\n",
        "from scapy.all import rdpcap, IP, TCP, UDP, DNS, DNSQR, ICMP, ARP, Raw\n",
        "from scapy.layers.http import HTTPRequest, HTTPResponse\n",
        "from collections import Counter, defaultdict\n",
        "from fpdf import FPDF\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from sklearn.cluster import DBSCAN, KMeans\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import socket\n",
        "from google.colab import files\n",
        "import nest_asyncio\n",
        "import dpkt\n",
        "import hashlib\n",
        "import re\n",
        "import pyclamd\n",
        "import yara\n",
        "import requests\n",
        "import time\n",
        "import subprocess\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import geocoder\n",
        "import qrcode\n",
        "from multiprocessing import Pool\n",
        "import threading\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import binascii\n",
        "import struct\n",
        "import io\n",
        "from datetime import datetime, timedelta\n",
        "from joblib import Parallel, delayed\n",
        "import cryptography  # Import cryptography explicitly for version check\n",
        "\n",
        "# Verify critical library versions (now after imports)\n",
        "print(f\"Scapy version: {scapy.__version__}\")\n",
        "print(f\"Cryptography version: {cryptography.__version__}\")\n",
        "\n",
        "# Check if pyod is available\n",
        "try:\n",
        "    from pyod.models.ecod import ECOD\n",
        "    PYOD_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYOD_AVAILABLE = False\n",
        "    print(\"Note: pyod library not available. Using alternative anomaly detection methods.\")\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Caches for efficiency\n",
        "ip_cache = {}\n",
        "dns_cache = {}\n",
        "geo_cache = {}\n",
        "threat_intel_cache = {}\n",
        "session_cache = {}\n",
        "\n",
        "class SnortAlert:\n",
        "    def __init__(self, sid, message, classification, priority, timestamp, src_ip, src_port, dst_ip, dst_port, protocol,\n",
        "                 ttl=None, payload=None, pkt_id=None, pkt_len=None, tos=None, flags=None, tcp_options=None,\n",
        "                 confidence=0.0, entropy=0.0, anomaly_score=0.0, threat_level=\"Low\", packet_hash=None):\n",
        "        self.sid = sid\n",
        "        self.message = message\n",
        "        self.classification = classification\n",
        "        self.priority = priority\n",
        "        self.timestamp = pd.to_datetime(timestamp, unit='s')\n",
        "        self.src_ip = src_ip\n",
        "        self.src_port = src_port\n",
        "        self.dst_ip = dst_ip\n",
        "        self.dst_port = dst_port\n",
        "        self.protocol = protocol\n",
        "        self.ttl = ttl\n",
        "        self.payload = payload\n",
        "        self.pkt_id = pkt_id\n",
        "        self.pkt_len = pkt_len\n",
        "        self.tos = tos\n",
        "        self.flags = flags\n",
        "        self.tcp_options = tcp_options\n",
        "        self.confidence = confidence\n",
        "        self.entropy = entropy\n",
        "        self.anomaly_score = anomaly_score\n",
        "        self.threat_level = threat_level\n",
        "        self.packet_hash = packet_hash\n",
        "\n",
        "MALWARE_SIGNATURES = {\n",
        "    \"Mirai\": [\"GET / HTTP/1.1\", \"User-Agent: Mirai\", b\"\\x00\\x01\"],\n",
        "    \"Zeus\": [\"POST /gate.php\", \"C&C handshake\", b\"\\xFF\\xEE\"],\n",
        "    \"WannaCry\": [\"SMBv1\", \"445\", b\"\\xFE\\xED\"],\n",
        "    \"Emotet\": [\"powershell\", \"I apologize\", b\"\\xDE\\xAD\"],\n",
        "    \"Qbot\": [\"random_file_name\", \"anti-analysis\", b\"\\xBE\\xEF\"],\n",
        "    \"Conficker\": [\"445/tcp\", \"rpc\", b\"\\xCA\\xFE\"],\n",
        "    \"NotPetya\": [\"PsExec\", \"EternalBlue\", b\"\\xBA\\xBE\"],\n",
        "    \"Generic Executable\": [b\"\\x4D\\x5A\"],\n",
        "    \"SQL Slammer\": [\"1434/udp\", \"buffer overflow\"],\n",
        "    \"Code Red\": [\"GET /default.ida\", \"NNNNNNNN\"],\n",
        "    \"Nimda\": [\".exe\", \"admin.dll\"],\n",
        "    \"SQL Injection\": [\"' OR 1=1\", \"SELECT * FROM\"],\n",
        "    \"TrickBot\": [\"banking\", \"inject\", b\"\\xAB\\xCD\"],\n",
        "    \"Ramnit\": [\"worm\", \"445\", b\"\\xEF\\xBE\"],\n",
        "}\n",
        "\n",
        "YARA_RULES = \"\"\"\n",
        "rule MiraiBotnet { strings: $a = \"Mirai\" nocase $b = \"GET / HTTP/1.1\" $c = {00 01} condition: any of them }\n",
        "rule WannaCry { strings: $a = \"SMBv1\" nocase $b = \"445\" $c = {FE ED} condition: any of them }\n",
        "rule Zeus { strings: $a = \"POST /gate.php\" $b = \"Zeus\" $c = {FF EE} condition: any of them }\n",
        "rule Emotet { strings: $a = \"powershell\" $b = \"Emotet\" $c = {DE AD} condition: any of them }\n",
        "rule Qbot { strings: $a = \"random_file_name\" $b = \"Qbot\" $c = {BE EF} condition: any of them }\n",
        "rule Conficker { strings: $a = \"445/tcp\" $b = \"rpc\" $c = {CA FE} condition: any of them }\n",
        "rule NotPetya { strings: $a = \"PsExec\" $b = \"NotPetya\" $c = {BA BE} condition: any of them }\n",
        "rule TrickBot { strings: $a = \"banking\" $b = \"inject\" $c = {AB CD} condition: any of them }\n",
        "rule Ramnit { strings: $a = \"worm\" $b = \"445\" $c = {EF BE} condition: any of them }\n",
        "rule GenericExecutable { strings: $a = {4D 5A} condition: $a }\n",
        "\"\"\"\n",
        "\n",
        "def upload_pcap():\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        return filename\n",
        "\n",
        "def resolve_ip(ip):\n",
        "    if ip in ip_cache:\n",
        "        return ip_cache[ip]\n",
        "    try:\n",
        "        result = socket.gethostbyaddr(ip)[0]\n",
        "    except:\n",
        "        result = \"Unknown\"\n",
        "    ip_cache[ip] = result\n",
        "    return result\n",
        "\n",
        "def geoip_lookup(ip):\n",
        "    if ip in geo_cache:\n",
        "        return geo_cache[ip]\n",
        "    try:\n",
        "        g = geocoder.ip(ip)\n",
        "        result = f\"{g.city}, {g.country}, Lat:{g.lat}, Lon:{g.lng}\" if g.ok else \"Unknown\"\n",
        "    except:\n",
        "        result = \"Unknown\"\n",
        "    geo_cache[ip] = result\n",
        "    return result\n",
        "\n",
        "def extract_features(pcap_file):\n",
        "    packets = rdpcap(pcap_file)\n",
        "    data = []\n",
        "    payloads = []\n",
        "    arp_count = 0\n",
        "    alerts = []\n",
        "    sid_counter = 1000001\n",
        "    tcp_states = defaultdict(lambda: {'syn': 0, 'syn_ack': 0, 'fin': 0, 'rst': 0})\n",
        "    flow_stats = defaultdict(lambda: {'packets': 0, 'bytes': 0, 'start_time': None, 'end_time': None})\n",
        "    timings = []\n",
        "    ip_pairs = Counter()\n",
        "\n",
        "    for i, pkt in enumerate(packets):\n",
        "        timestamp = float(pkt.time)\n",
        "        if i > 0:\n",
        "            timings.append(timestamp - float(packets[i-1].time))\n",
        "\n",
        "        if pkt.haslayer(IP):\n",
        "            src_ip = pkt[IP].src\n",
        "            dst_ip = pkt[IP].dst\n",
        "            length = len(pkt)\n",
        "            protocol = pkt[IP].proto if pkt.haslayer(IP) else \"Unknown\"\n",
        "            src_port = dst_port = flags = ttl = tos = tcp_options = None\n",
        "            payload = pkt[IP].payload if pkt.haslayer(IP) and hasattr(pkt[IP], 'payload') else None\n",
        "            pkt_id = pkt[IP].id if pkt.haslayer(IP) and hasattr(pkt[IP], 'id') else None\n",
        "            ip_pairs[(src_ip, dst_ip)] += 1\n",
        "\n",
        "            if pkt.haslayer(TCP):\n",
        "                protocol = \"TCP\"\n",
        "                src_port = pkt[TCP].sport\n",
        "                dst_port = pkt[TCP].dport\n",
        "                flags = str(pkt[TCP].flags)\n",
        "                ttl = pkt[IP].ttl\n",
        "                tos = pkt[IP].tos\n",
        "                tcp_options = pkt[TCP].options if pkt[TCP].options else \"None\"\n",
        "                flow_key = (src_ip, dst_ip, src_port, dst_port, protocol)\n",
        "                if 'S' in flags and not 'A' in flags:\n",
        "                    tcp_states[flow_key]['syn'] += 1\n",
        "                    message = \"ET SCAN Potential SSH Scan\" if dst_port == 22 else \"TCP SYN Scan\"\n",
        "                    alerts.append(SnortAlert(sid_counter, message, \"Attempted Recon\", 2, timestamp, src_ip, src_port, dst_ip, dst_port, protocol, ttl, bytes(payload) if payload else None, pkt_id, length, tos, flags, tcp_options))\n",
        "                    sid_counter += 1\n",
        "                if 'SA' in flags:\n",
        "                    tcp_states[flow_key]['syn_ack'] += 1\n",
        "                if 'F' in flags:\n",
        "                    tcp_states[flow_key]['fin'] += 1\n",
        "                if 'R' in flags:\n",
        "                    tcp_states[flow_key]['rst'] += 1\n",
        "                if dst_port == 445 and payload and b\"SMB\" in bytes(payload):\n",
        "                    alerts.append(SnortAlert(sid_counter, \"WannaCry SMB Exploit\", \"Malware\", 1, timestamp, src_ip, src_port, dst_ip, dst_port, protocol, ttl, bytes(payload), pkt_id, length, tos, flags, tcp_options))\n",
        "                    sid_counter += 1\n",
        "            elif pkt.haslayer(UDP):\n",
        "                protocol = \"UDP\"\n",
        "                src_port = pkt[UDP].sport\n",
        "                dst_port = pkt[UDP].dport\n",
        "                ttl = pkt[IP].ttl\n",
        "                tos = pkt[IP].tos\n",
        "                flow_key = (src_ip, dst_ip, src_port, dst_port, protocol)\n",
        "            elif pkt.haslayer(ICMP):\n",
        "                protocol = \"ICMP\"\n",
        "                ttl = pkt[IP].ttl\n",
        "                tos = pkt[IP].tos\n",
        "                flow_key = (src_ip, dst_ip, None, None, protocol)\n",
        "            else:\n",
        "                protocol = \"Other\"\n",
        "                flow_key = (src_ip, dst_ip, None, None, protocol)\n",
        "\n",
        "            if flow_stats[flow_key]['start_time'] is None:\n",
        "                flow_stats[flow_key]['start_time'] = timestamp\n",
        "            flow_stats[flow_key]['end_time'] = timestamp\n",
        "            flow_stats[flow_key]['packets'] += 1\n",
        "            flow_stats[flow_key]['bytes'] += length\n",
        "            data.append([timestamp, src_ip, dst_ip, protocol, length, src_port, dst_port, flags, ttl, tos, pkt_id])\n",
        "            if payload:\n",
        "                payloads.append(bytes(payload))\n",
        "\n",
        "        elif pkt.haslayer(ARP):\n",
        "            arp_count += 1\n",
        "\n",
        "    df = pd.DataFrame(data, columns=[\"Timestamp\", \"Source_IP\", \"Destination_IP\", \"Protocol\", \"Packet_Length\", \"Source_Port\", \"Destination_Port\", \"Flags\", \"TTL\", \"TOS\", \"Packet_ID\"])\n",
        "    return df, packets, payloads, arp_count, alerts, tcp_states, flow_stats, timings, ip_pairs\n",
        "\n",
        "def abuseipdb_lookup(ip, api_key=\"YOUR-KEY\"):\n",
        "    if ip in threat_intel_cache:\n",
        "        return threat_intel_cache[ip]\n",
        "    url = f\"https://api.abuseipdb.com/api/v2/check?ipAddress={ip}&maxAgeInDays=90\"\n",
        "    headers = {\"Key\": api_key, \"Accept\": \"application/json\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()['data']\n",
        "            result = f\"Confidence {data['abuseConfidenceScore']}%, Reports: {data['totalReports']}, Last Reported: {data.get('lastReportedAt', 'N/A')}\"\n",
        "            threat_intel_cache[ip] = result\n",
        "            return result\n",
        "        return f\"Failed: Status {response.status_code}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "    time.sleep(1)\n",
        "\n",
        "def advanced_zero_day_detection(df):\n",
        "    features = df[['Packet_Length', 'TTL', 'TOS', 'Packet_ID']].fillna(0)\n",
        "    scaler = RobustScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    input_dim = features_scaled.shape[1]\n",
        "    encoder = tf.keras.Sequential([layers.Input(shape=(input_dim,)), layers.Dense(32, activation='relu'), layers.Dense(16, activation='relu'), layers.Dense(8)])\n",
        "    decoder = tf.keras.Sequential([layers.Input(shape=(8,)), layers.Dense(16, activation='relu'), layers.Dense(32, activation='relu'), layers.Dense(input_dim, activation='sigmoid')])\n",
        "    inputs = layers.Input(shape=(input_dim,))\n",
        "    encoded = encoder(inputs)\n",
        "    z_mean = layers.Dense(8)(encoded)\n",
        "    z_log_var = layers.Dense(8)(encoded)\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], 8))\n",
        "    z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "    outputs = decoder(z)\n",
        "    vae = tf.keras.Model(inputs, outputs)\n",
        "    reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.binary_crossentropy(inputs, outputs), axis=-1))\n",
        "    kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
        "    vae.add_loss(reconstruction_loss + kl_loss)\n",
        "    vae.compile(optimizer='adam')\n",
        "    vae.fit(features_scaled, epochs=5, batch_size=128, shuffle=True, verbose=0)\n",
        "    reconstructions = vae.predict(features_scaled, verbose=0)\n",
        "    mse = np.mean(np.power(features_scaled - reconstructions, 2), axis=1)\n",
        "    vae_threshold = np.percentile(mse, 97)\n",
        "    vae_anomalies = mse > vae_threshold\n",
        "\n",
        "    if PYOD_AVAILABLE:\n",
        "        ecod = ECOD()\n",
        "        ecod_scores = ecod.fit_predict(features_scaled)\n",
        "        ecod_anomalies = ecod_scores == 1\n",
        "        combined_anomalies = vae_anomalies | ecod_anomalies\n",
        "    else:\n",
        "        combined_anomalies = vae_anomalies\n",
        "        print(\"Using VAE only for zero-day detection\")\n",
        "\n",
        "    return combined_anomalies.sum(), df.loc[combined_anomalies, 'Source_IP'].unique().tolist(), mse\n",
        "\n",
        "def advanced_behavioral_profiling(df):\n",
        "    features = df[['Packet_Length', 'TTL', 'TOS', 'Packet_ID']].fillna(0)\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    dbscan = DBSCAN(eps=0.7, min_samples=5, n_jobs=-1).fit(features_scaled)\n",
        "    df['DBSCAN_Cluster'] = dbscan.labels_\n",
        "\n",
        "    kmeans = KMeans(n_clusters=5, random_state=42, n_init=10).fit(features_scaled)\n",
        "    df['KMeans_Cluster'] = kmeans.labels_\n",
        "\n",
        "    suspicious_dbscan = df[df['DBSCAN_Cluster'] == -1]['Source_IP'].unique().tolist()\n",
        "    suspicious_kmeans = df.groupby('KMeans_Cluster')['Packet_Length'].mean().idxmax()\n",
        "    suspicious_kmeans_ips = df[df['KMeans_Cluster'] == suspicious_kmeans]['Source_IP'].unique().tolist()\n",
        "\n",
        "    return suspicious_dbscan, suspicious_kmeans_ips\n",
        "\n",
        "def calculate_entropy(payload):\n",
        "    if len(payload) == 0:\n",
        "        return 0\n",
        "    byte_counts = Counter(payload)\n",
        "    entropy = -sum((count / len(payload)) * math.log2(count / len(payload)) for count in byte_counts.values())\n",
        "    return entropy\n",
        "\n",
        "def detect_anomalies(df):\n",
        "    features = df[['Packet_Length', 'TTL', 'TOS', 'Packet_ID']].fillna(0)\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    iso_forest = IsolationForest(contamination=0.05, random_state=42, n_jobs=-1)\n",
        "    df['Anomaly_Score_Iso'] = iso_forest.fit_predict(features_scaled)\n",
        "\n",
        "    xgb_model = xgb.XGBClassifier(random_state=42, n_jobs=-1)\n",
        "    xgb_model.fit(features_scaled, df['Anomaly_Score_Iso'] == -1)\n",
        "    df['Anomaly_Score_XGB'] = xgb_model.predict_proba(features_scaled)[:, 1]\n",
        "\n",
        "    return df\n",
        "\n",
        "def analyze_packet_timing(timings):\n",
        "    if not timings:\n",
        "        return \"No timing data\", 0, 0\n",
        "    mean_delay = np.mean(timings)\n",
        "    std_delay = np.std(timings)\n",
        "    anomalies = sum(1 for t in timings if abs(t - mean_delay) > 3 * std_delay)\n",
        "    jitter = np.var(timings)\n",
        "    return \"Timing anomalies detected\" if anomalies > 10 else \"Normal timing\", anomalies, jitter\n",
        "\n",
        "def detect_attack_types(df, arp_count, alerts, tcp_states, flow_stats, ip_pairs):\n",
        "    attack_types = set()\n",
        "    src_ip_counts = df['Source_IP'].value_counts()\n",
        "    if src_ip_counts.max() > 1500:\n",
        "        attack_types.add(f\"DDoS Attack (High packet count from {src_ip_counts.idxmax()}: {src_ip_counts.max()})\")\n",
        "    for flow, stats in flow_stats.items():\n",
        "        duration = stats['end_time'] - stats['start_time']\n",
        "        if duration > 0 and stats['bytes'] / duration > 5000:\n",
        "            attack_types.add(f\"Data Exfiltration (Flow: {flow[0]}:{flow[2]} -> {flow[1]}:{flow[3]}, {stats['bytes']/duration:.2f} B/s)\")\n",
        "    for (src, dst), count in ip_pairs.most_common(5):\n",
        "        if count > 1000:\n",
        "            attack_types.add(f\"High Traffic Pair: {src} -> {dst} ({count} packets)\")\n",
        "    for alert in alerts:\n",
        "        attack_types.add(f\"{alert.classification}: {alert.message}\")\n",
        "    return list(attack_types) if attack_types else [\"No specific attack detected\"]\n",
        "\n",
        "def hash_payloads(payloads):\n",
        "    return [(hashlib.md5(p).hexdigest(), hashlib.sha256(p).hexdigest(), hashlib.sha1(p).hexdigest()) for p in payloads[:15]]\n",
        "\n",
        "def virustotal_lookup(hashes, api_key=\"YOUR-KEY\"):\n",
        "    if not api_key or api_key == \"No KEY Available\":\n",
        "        return [\"VirusTotal lookup skipped (no API key provided)\"]\n",
        "    url = \"https://www.virustotal.com/api/v3/files/\"\n",
        "    headers = {\"x-apikey\": api_key}\n",
        "    results = []\n",
        "    for md5, sha256, sha1 in hashes[:5]:\n",
        "        response = requests.get(url + sha256, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            positives = data['data']['attributes']['last_analysis_stats']['malicious']\n",
        "            if positives > 0:\n",
        "                results.append(f\"Hash {sha256}: Malicious ({positives} engines)\")\n",
        "        time.sleep(15)\n",
        "    return results if results else [\"No malicious hashes found\"]\n",
        "\n",
        "def otx_lookup(ip, api_key=\"YOUR-KEY\"):\n",
        "    if ip in threat_intel_cache:\n",
        "        return threat_intel_cache[ip]\n",
        "    url = f\"https://otx.alienvault.com/api/v1/indicators/IPv4/{ip}/general\"\n",
        "    headers = {\"X-OTX-API-KEY\": api_key}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            pulse_count = len(data.get('pulse_info', {}).get('pulses', []))\n",
        "            reputation = data.get('reputation', 0)\n",
        "            result = f\"Pulses: {pulse_count}, Reputation: {reputation}\"\n",
        "            threat_intel_cache[ip] = result\n",
        "            return result\n",
        "        return f\"Failed: Status {response.status_code}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "    time.sleep(1)\n",
        "\n",
        "def clamav_scan_worker(payload_data):\n",
        "    i, payload = payload_data\n",
        "    temp_file = f\"temp_payload_{i}.bin\"\n",
        "    with open(temp_file, \"wb\") as f:\n",
        "        f.write(payload)\n",
        "    result = subprocess.run(['clamscan', '--no-summary', temp_file], capture_output=True, text=True)\n",
        "    os.remove(temp_file)\n",
        "    return f\"ClamAV detected malware in payload {i}: {result.stdout.splitlines()[0]}\" if \"Infected files: 1\" in result.stdout else None\n",
        "\n",
        "def clamav_scan(payloads):\n",
        "    with Pool(4) as pool:\n",
        "        results = pool.map(clamav_scan_worker, [(i, p) for i, p in enumerate(payloads[:15])])\n",
        "    return [r for r in results if r] or [\"No malware detected by ClamAV\"]\n",
        "\n",
        "def yara_scan(payloads):\n",
        "    with open(\"temp_yara_rules.yar\", \"w\") as f:\n",
        "        f.write(YARA_RULES)\n",
        "    rules = yara.compile(filepath=\"temp_yara_rules.yar\")\n",
        "\n",
        "    detection_counts = Counter()\n",
        "    payload_details = defaultdict(list)\n",
        "\n",
        "    for i, payload in enumerate(payloads[:15]):\n",
        "        temp_file = f\"temp_payload_{i}.bin\"\n",
        "        with open(temp_file, \"wb\") as f:\n",
        "            f.write(payload)\n",
        "\n",
        "        matches = rules.match(temp_file)\n",
        "        os.remove(temp_file)\n",
        "\n",
        "        for match in matches:\n",
        "            detection_counts[match.rule] += 1\n",
        "            payload_details[match.rule].append(i)\n",
        "\n",
        "    os.remove(\"temp_yara_rules.yar\")\n",
        "\n",
        "    results = []\n",
        "    for rule, count in detection_counts.most_common():\n",
        "        examples = payload_details[rule][:3]\n",
        "        examples_str = \", \".join(f\"payload {n}\" for n in examples)\n",
        "        if count > 3:\n",
        "            examples_str += f\" (+{count-3} more)\"\n",
        "        results.append(f\"{rule}: {count} ({examples_str})\")\n",
        "    return results if results else [\"No YARA matches found\"]\n",
        "\n",
        "def detect_malware(payloads):\n",
        "    malware_detected = []\n",
        "    for i, payload in enumerate(payloads):\n",
        "        payload_str = str(payload)\n",
        "        payload_bytes = bytes(payload)\n",
        "        entropy = calculate_entropy(payload_bytes)\n",
        "        for name, signatures in MALWARE_SIGNATURES.items():\n",
        "            for sig in signatures:\n",
        "                if isinstance(sig, str) and sig in payload_str:\n",
        "                    malware_detected.append((name, f\"Signature: {sig}\", entropy, i))\n",
        "                elif isinstance(sig, bytes) and sig in payload_bytes:\n",
        "                    malware_detected.append((name, \"Binary Signature\", entropy, i))\n",
        "    return Counter([(m[0], m[1]) for m in malware_detected]).most_common()\n",
        "\n",
        "def extract_dns_queries(packets):\n",
        "    dns_queries = []\n",
        "    for pkt in packets:\n",
        "        if pkt.haslayer(DNS) and pkt.haslayer(DNSQR):\n",
        "            query = pkt[DNSQR].qname.decode('utf-8', errors='ignore')\n",
        "            if pkt.haslayer(IP):\n",
        "                dns_queries.append((query, pkt[IP].src, pkt[IP].dst))\n",
        "    return Counter([(q[0], q[1], q[2]) for q in dns_queries]).most_common(15)\n",
        "\n",
        "def application_layer_decoding(packets):\n",
        "    http_requests = []\n",
        "    smtp_data = []\n",
        "    ftp_commands = []\n",
        "    for pkt in packets:\n",
        "        if pkt.haslayer(TCP) and pkt[TCP].dport == 80 and pkt.haslayer(Raw):\n",
        "            payload = bytes(pkt[Raw])\n",
        "            if b\"GET\" in payload or b\"POST\" in payload:\n",
        "                http_requests.append(payload.decode('utf-8', errors='ignore').split('\\r\\n')[0])\n",
        "        elif pkt.haslayer(TCP) and pkt[TCP].dport == 25 and pkt.haslayer(Raw):\n",
        "            smtp_data.append(bytes(pkt[Raw]).decode('utf-8', errors='ignore')[:100])\n",
        "        elif pkt.haslayer(TCP) and pkt[TCP].dport == 21 and pkt.haslayer(Raw):\n",
        "            ftp_commands.append(bytes(pkt[Raw]).decode('utf-8', errors='ignore')[:50])\n",
        "    return http_requests[:10], smtp_data[:10], ftp_commands[:10]\n",
        "\n",
        "def encrypted_traffic_analysis(df, payloads):\n",
        "    tls_df = df[(df['Protocol'] == 'TCP') & ((df['Destination_Port'].isin([443, 8443])) | (df['Source_Port'].isin([443, 8443])))]\n",
        "    if tls_df.empty:\n",
        "        return \"No TLS traffic detected\", 0, 0.0\n",
        "    avg_size = tls_df['Packet_Length'].mean()\n",
        "    anomaly_count = len(tls_df[tls_df['Anomaly_Score_Iso'] == -1])\n",
        "    tls_payloads = [payloads[i] for i in tls_df.index if i < len(payloads)]\n",
        "    if tls_payloads:\n",
        "        concatenated_payload = b''.join(tls_payloads[:10])\n",
        "        entropy_avg = calculate_entropy(concatenated_payload)\n",
        "    else:\n",
        "        entropy_avg = 0.0\n",
        "    return (\"Suspicious TLS traffic detected\" if anomaly_count > 10 or avg_size > 1500 else \"Normal TLS traffic\"), anomaly_count, entropy_avg\n",
        "\n",
        "def ml_threat_scoring(df, alerts, entropy_info):\n",
        "    if not alerts or len(df) < 10:\n",
        "        return alerts\n",
        "\n",
        "    features = df[['Packet_Length', 'TTL', 'TOS', 'Packet_ID', 'Anomaly_Score_XGB']].fillna(0)\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(features)\n",
        "\n",
        "    alert_ips = set(a.src_ip for a in alerts)\n",
        "    y = np.array([1 if ip in alert_ips else 0 for ip in df['Source_IP']])\n",
        "\n",
        "    pos_weight = (len(y) - sum(y)) / max(sum(y), 1)\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=50, max_depth=8, min_samples_split=10, n_jobs=-1, random_state=42)\n",
        "    xgb_model = xgb.XGBClassifier(max_depth=4, scale_pos_weight=pos_weight, n_estimators=50, learning_rate=0.1, n_jobs=-1, random_state=42, eval_metric='logloss')\n",
        "    lgb_model = lgb.LGBMClassifier(n_estimators=50, max_depth=4, min_child_samples=10, min_child_weight=0.001, num_leaves=10, learning_rate=0.1, n_jobs=-1, random_state=42)\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "    lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)])\n",
        "\n",
        "    rf_scores = rf.predict_proba(X)[:, 1]\n",
        "    xgb_scores = xgb_model.predict_proba(X)[:, 1]\n",
        "    lgb_scores = lgb_model.predict_proba(X)[:, 1]\n",
        "\n",
        "    ensemble_scores = (0.4 * xgb_scores + 0.3 * rf_scores + 0.3 * lgb_scores)\n",
        "\n",
        "    for i, alert in enumerate(alerts):\n",
        "        idx = df[(df['Source_IP'] == alert.src_ip) & (df['Timestamp'] == alert.timestamp)].index\n",
        "        if len(idx) > 0:\n",
        "            score = ensemble_scores[idx[0]]\n",
        "            alert.confidence = score\n",
        "            alert.threat_level = \"Critical\" if score > 0.9 else \"High\" if score > 0.7 else \"Medium\" if score > 0.5 else \"Low\"\n",
        "        else:\n",
        "            alert.confidence = 0.3\n",
        "\n",
        "    return alerts\n",
        "\n",
        "def cluster_threat_correlation(alerts):\n",
        "    if len(alerts) < 3:\n",
        "        return \"Insufficient data for clustering\", 0\n",
        "    ip_times = [(a.src_ip, a.timestamp.timestamp()) for a in alerts]\n",
        "    df = pd.DataFrame(ip_times, columns=['IP', 'Time'])\n",
        "    clustering = DBSCAN(eps=300, min_samples=3, n_jobs=-1).fit(df[['Time']])\n",
        "    clusters = Counter(clustering.labels_)\n",
        "    if -1 in clusters:\n",
        "        del clusters[-1]\n",
        "    return f\"Correlated threats in {len(clusters)} clusters\", len(clusters)\n",
        "\n",
        "def generate_advanced_visuals(df):\n",
        "    sns.set(style='whitegrid')\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    sns.histplot(df['Packet_Length'], bins=100, kde=True, color='blue')\n",
        "    plt.title('Packet Length Distribution')\n",
        "    plt.savefig(\"length_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    protocol_counts = df['Protocol'].value_counts()\n",
        "    sns.barplot(x=protocol_counts.index, y=protocol_counts.values)\n",
        "    for i, v in enumerate(protocol_counts.values):\n",
        "        plt.text(i, v, f\"{v/sum(protocol_counts.values)*100:.1f}%\", ha='center', va='bottom')\n",
        "    plt.title('Protocol Distribution')\n",
        "    plt.savefig(\"protocol_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    sns.scatterplot(data=df, x='Timestamp', y='Packet_Length', hue='Anomaly_Score_Iso', palette={1: 'blue', -1: 'red'}, size='Anomaly_Score_XGB')\n",
        "    plt.title('Anomaly Detection Over Time')\n",
        "    plt.savefig(\"anomaly_timeline.png\")\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    heatmap_data = df.pivot_table(index='Source_IP', columns='Destination_Port', values='Packet_Length', aggfunc='count', fill_value=0)\n",
        "    sns.heatmap(heatmap_data, cmap=\"YlOrRd\", annot=False)\n",
        "    plt.title('Threat Heatmap (Source IP vs Destination Port)')\n",
        "    plt.savefig(\"threat_heatmap.png\")\n",
        "    plt.close()\n",
        "\n",
        "    features = df[['Packet_Length', 'TTL', 'TOS', 'Packet_ID']].fillna(0)\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(features)\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=df['Anomaly_Score_Iso'], cmap='coolwarm')\n",
        "    plt.title('PCA of Network Features')\n",
        "    plt.savefig(\"pca_visual.png\")\n",
        "    plt.close()\n",
        "\n",
        "def generate_firewall_rules(df, attack_types, high_rate_ips):\n",
        "    rules = {'iptables': [], 'aws_waf': [], 'azure': [], 'snort': []}\n",
        "    for ip in high_rate_ips:\n",
        "        rules['iptables'].append(f\"iptables -A INPUT -s {ip} -j DROP  # Block {ip}\")\n",
        "        rules['aws_waf'].append(f\"Block IP {ip} in AWS WAF\")\n",
        "        rules['azure'].append(f\"Deny IP {ip} in Azure Firewall\")\n",
        "        rules['snort'].append(f\"drop ip {ip} any -> any any (msg:\\\"Blocked high-rate IP {ip}\\\"; sid:{1000000 + len(rules['snort'])};)\")\n",
        "    if any(\"DDoS\" in attack for attack in attack_types):\n",
        "        top_ip = df['Source_IP'].value_counts().idxmax()\n",
        "        rules['iptables'].append(f\"iptables -A INPUT -s {top_ip} -m limit --limit 50/s -j ACCEPT  # Rate limit {top_ip}\")\n",
        "        rules['snort'].append(f\"alert ip {top_ip} any -> any any (msg:\\\"DDoS from {top_ip}\\\"; sid:{1000000 + len(rules['snort'])};)\")\n",
        "    return rules\n",
        "\n",
        "def carve_files_from_payloads(payloads, output_dir=\"carved_files\"):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    carved_files = []\n",
        "    file_signatures = {\"PDF\": b\"%PDF-\", \"PNG\": b\"\\x89PNG\", \"JPEG\": b\"\\xFF\\xD8\\xFF\", \"EXE\": b\"MZ\", \"ZIP\": b\"PK\\x03\\x04\"}\n",
        "\n",
        "    for i, payload in enumerate(payloads[:50]):\n",
        "        payload_bytes = bytes(payload)\n",
        "        for file_type, sig in file_signatures.items():\n",
        "            if sig in payload_bytes:\n",
        "                start_idx = payload_bytes.index(sig)\n",
        "                end_idx = min(start_idx + 1048576, len(payload_bytes))\n",
        "                file_data = payload_bytes[start_idx:end_idx]\n",
        "                file_hash = hashlib.sha256(file_data).hexdigest()[:8]\n",
        "                filename = f\"{output_dir}/{file_type.lower()}_{file_hash}.{file_type.lower()}\"\n",
        "                with open(filename, \"wb\") as f:\n",
        "                    f.write(file_data)\n",
        "                carved_files.append((file_type, filename, len(file_data)))\n",
        "                yara_matches = yara_scan([file_data])\n",
        "                if yara_matches != [\"No YARA matches found\"]:\n",
        "                    carved_files[-1] = carved_files[-1] + (yara_matches,)\n",
        "    return carved_files\n",
        "\n",
        "def reconstruct_tcp_sessions(packets):\n",
        "    sessions = defaultdict(list)\n",
        "    reconstructed_data = {}\n",
        "    for pkt in packets:\n",
        "        if pkt.haslayer(TCP) and pkt.haslayer(IP):\n",
        "            flow_key = (pkt[IP].src, pkt[TCP].sport, pkt[IP].dst, pkt[TCP].dport)\n",
        "            sessions[flow_key].append(pkt)\n",
        "    for flow_key, pkt_list in sessions.items():\n",
        "        pkt_list.sort(key=lambda x: x[TCP].seq)\n",
        "        data_stream = b\"\"\n",
        "        last_ack = 0\n",
        "        for pkt in pkt_list:\n",
        "            if pkt.haslayer(Raw):\n",
        "                payload = bytes(pkt[Raw])\n",
        "                if pkt[TCP].seq > last_ack:\n",
        "                    data_stream += payload\n",
        "                    last_ack = pkt[TCP].seq + len(payload)\n",
        "        if data_stream:\n",
        "            reconstructed_data[flow_key] = data_stream\n",
        "    return reconstructed_data\n",
        "\n",
        "def deep_packet_inspection(packets):\n",
        "    http_sessions = []\n",
        "    suspicious_protocols = []\n",
        "    for pkt in packets:\n",
        "        if pkt.haslayer(HTTPRequest):\n",
        "            req = pkt[HTTPRequest]\n",
        "            http_sessions.append({\n",
        "                \"Type\": \"Request\",\n",
        "                \"Method\": req.Method.decode('utf-8', errors='ignore') if hasattr(req, 'Method') else \"N/A\",\n",
        "                \"Host\": req.Host.decode('utf-8', errors='ignore') if hasattr(req, 'Host') else \"N/A\",\n",
        "                \"Path\": req.Path.decode('utf-8', errors='ignore') if hasattr(req, 'Path') else \"N/A\",\n",
        "                \"Src_IP\": pkt[IP].src if pkt.haslayer(IP) else \"N/A\",\n",
        "                \"Dst_IP\": pkt[IP].dst if pkt.haslayer(IP) else \"N/A\"\n",
        "            })\n",
        "        elif pkt.haslayer(HTTPResponse):\n",
        "            resp = pkt[HTTPResponse]\n",
        "            raw = resp.original if hasattr(resp, 'original') else b''\n",
        "            status_line = raw.split(b'\\r\\n')[0].decode('utf-8', errors='ignore') if raw else \"N/A\"\n",
        "            http_sessions.append({\n",
        "                \"Type\": \"Response\",\n",
        "                \"Status\": status_line,\n",
        "                \"Src_IP\": pkt[IP].src if pkt.haslayer(IP) else \"N/A\",\n",
        "                \"Dst_IP\": pkt[IP].dst if pkt.haslayer(IP) else \"N/A\"\n",
        "            })\n",
        "    return http_sessions[:10], suspicious_protocols[:5]\n",
        "\n",
        "def simulate_memory_artifacts(payloads):\n",
        "    memory_artifacts = []\n",
        "    for i, payload in enumerate(payloads[:15]):\n",
        "        payload_bytes = bytes(payload)\n",
        "        strings = re.findall(b\"[ -~]{4,}\", payload_bytes)\n",
        "        if strings:\n",
        "            memory_artifacts.append({\"Payload_ID\": i, \"Strings\": [s.decode('ascii', errors='ignore') for s in strings[:5]], \"Entropy\": calculate_entropy(payload_bytes)\n",
        "            })\n",
        "        for offset in range(0, min(len(payload_bytes), 256), 4):\n",
        "            chunk = payload_bytes[offset:offset+4]\n",
        "            if len(chunk) == 4:\n",
        "                try:\n",
        "                    val = struct.unpack(\"<I\", chunk)[0]\n",
        "                    if 0x1000 <= val <= 0x7FFFFFFF:\n",
        "                        memory_artifacts.append({\"Payload_ID\": i, \"Pointer\": hex(val), \"Offset\": offset})\n",
        "                except struct.error:\n",
        "                    continue\n",
        "    return memory_artifacts\n",
        "\n",
        "def forensic_timeline_analysis(df, alerts):\n",
        "    timeline_events = []\n",
        "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
        "    for idx, row in df.iterrows():\n",
        "        if row['Anomaly_Score_Iso'] == -1:\n",
        "            timeline_events.append({\"Time\": row['Timestamp'], \"Event\": f\"Anomalous Packet from {row['Source_IP']} to {row['Destination_IP']}\", \"Details\": f\"Length: {row['Packet_Length']}, Protocol: {row['Protocol']}\"})\n",
        "    for alert in alerts:\n",
        "        timeline_events.append({\"Time\": alert.timestamp, \"Event\": f\"Alert: {alert.message}\", \"Details\": f\"SID: {alert.sid}, Confidence: {alert.confidence:.2f}, Threat: {alert.threat_level}\"})\n",
        "    timeline_events.sort(key=lambda x: x[\"Time\"])\n",
        "    return timeline_events\n",
        "\n",
        "def network_entropy_analysis(payloads, df):\n",
        "    entropy_values = Parallel(n_jobs=-1)(delayed(calculate_entropy)(p) for p in payloads[:100])\n",
        "    df_entropy = pd.DataFrame({'Timestamp': df['Timestamp'][:len(entropy_values)], 'Entropy': entropy_values})\n",
        "    entropy_trend = df_entropy.groupby(df_entropy['Timestamp'].dt.floor('5min'))['Entropy'].mean()\n",
        "    high_entropy_spikes = entropy_trend[entropy_trend > 6].count()\n",
        "    return entropy_trend, high_entropy_spikes\n",
        "\n",
        "def threat_correlation_matrix(alerts):\n",
        "    if len(alerts) < 2:\n",
        "        return None\n",
        "    alert_df = pd.DataFrame([(a.src_ip, a.dst_ip, a.timestamp.timestamp()) for a in alerts], columns=['Src_IP', 'Dst_IP', 'Time'])\n",
        "    correlation_matrix = alert_df.pivot_table(index='Src_IP', columns='Dst_IP', values='Time', aggfunc='count', fill_value=0)\n",
        "    return correlation_matrix\n",
        "\n",
        "# New NIDS Feature: Protocol Anomaly Detection\n",
        "def detect_protocol_anomalies(packets, alerts, sid_counter):\n",
        "    unusual_flags = Counter()\n",
        "    malformed_packets = 0\n",
        "    for pkt in packets:\n",
        "        if pkt.haslayer(TCP):\n",
        "            flags = str(pkt[TCP].flags)\n",
        "            if flags not in ['S', 'SA', 'A', 'F', 'FA', 'PA', 'R', 'RA']:\n",
        "                unusual_flags[flags] += 1\n",
        "                alerts.append(SnortAlert(\n",
        "                    sid_counter, f\"Unusual TCP Flags: {flags}\", \"Protocol Anomaly\", 2, float(pkt.time),\n",
        "                    pkt[IP].src, pkt[TCP].sport, pkt[IP].dst, pkt[TCP].dport, \"TCP\"\n",
        "                ))\n",
        "                sid_counter += 1\n",
        "        if pkt.haslayer(IP) and pkt[IP].len < 20:  # Minimum IP header length\n",
        "            malformed_packets += 1\n",
        "            alerts.append(SnortAlert(\n",
        "                sid_counter, \"Malformed IP Header\", \"Protocol Anomaly\", 3, float(pkt.time),\n",
        "                pkt[IP].src, None, pkt[IP].dst, None, \"IP\"\n",
        "            ))\n",
        "            sid_counter += 1\n",
        "    return unusual_flags.most_common(5), malformed_packets, alerts, sid_counter\n",
        "\n",
        "# New NIDS Feature: Session Hijacking Detection\n",
        "def detect_session_hijacking(packets, tcp_states, alerts, sid_counter):\n",
        "    seq_anomalies = []\n",
        "    for flow_key, states in tcp_states.items():\n",
        "        if states['rst'] > 5 or (states['syn'] > 1 and states['syn_ack'] == 0):\n",
        "            src_ip, dst_ip, src_port, dst_port, _ = flow_key\n",
        "            alerts.append(SnortAlert(\n",
        "                sid_counter, \"Potential Session Hijacking\", \"Session Anomaly\", 2, time.time(),\n",
        "                src_ip, src_port, dst_ip, dst_port, \"TCP\"\n",
        "            ))\n",
        "            sid_counter += 1\n",
        "            seq_anomalies.append(flow_key)\n",
        "    return seq_anomalies, alerts, sid_counter\n",
        "\n",
        "# New NIDS Feature: Covert Channel Detection\n",
        "def detect_covert_channels(packets, payloads, alerts, sid_counter):\n",
        "    dns_tunneling = []\n",
        "    for pkt in packets:\n",
        "        if pkt.haslayer(DNSQR):\n",
        "            qname = pkt[DNSQR].qname.decode('utf-8', errors='ignore')\n",
        "            if len(qname) > 100 or calculate_entropy(qname.encode()) > 5:\n",
        "                dns_tunneling.append((qname, pkt[IP].src, pkt[IP].dst))\n",
        "                alerts.append(SnortAlert(\n",
        "                    sid_counter, \"DNS Tunneling Suspected\", \"Covert Channel\", 2, float(pkt.time),\n",
        "                    pkt[IP].src, None, pkt[IP].dst, None, \"UDP\"\n",
        "                ))\n",
        "                sid_counter += 1\n",
        "    return dns_tunneling[:5], alerts, sid_counter\n",
        "\n",
        "# New Function: Nikto Scan and PDF Generation\n",
        "def nikto_scan(url):\n",
        "    try:\n",
        "        result = subprocess.run(['nikto', '-h', url], capture_output=True, text=True)\n",
        "        nikto_output = result.stdout\n",
        "    except Exception as e:\n",
        "        nikto_output = f\"Error running Nikto scan: {str(e)}\"\n",
        "\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", 'B', 16)\n",
        "    pdf.cell(0, 10, \"Nikto Vulnerability Scan Report\", ln=True, align='C')\n",
        "    pdf.ln(10)\n",
        "    pdf.set_font(\"Courier\", size=10)\n",
        "    pdf.multi_cell(0, 6, f\"URL Scanned: {url}\\n\\n{nikto_output}\")\n",
        "\n",
        "    pdf_output_path = \"NETSYN_Vulnerability_Report.pdf\"\n",
        "    pdf.output(pdf_output_path)\n",
        "    if os.path.exists(pdf_output_path) and os.path.getsize(pdf_output_path) > 0:\n",
        "        files.download(pdf_output_path)\n",
        "        print(f\"Nikto report downloaded as '{pdf_output_path}'\")\n",
        "    else:\n",
        "        print(\"Error: Nikto PDF file was not generated or is empty.\")\n",
        "    return nikto_output\n",
        "\n",
        "def generate_report(df, packets, payloads, attack_types, malware_detected, clamav_results, yara_results, vt_results, dns_queries,\n",
        "                    high_rate_ips, arp_count, alerts, entropy_info, tcp_states, flow_stats, timings, ip_pairs,\n",
        "                    carved_files, tcp_sessions, http_sessions, suspicious_protocols, memory_artifacts, timeline_events,\n",
        "                    abuseipdb_results, unusual_flags, malformed_packets, seq_anomalies, covert_channels):\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "\n",
        "    # Page Setup\n",
        "    page_width = 210 - 20  # A4 width minus margins (10mm each side)\n",
        "    pdf.set_font(\"Arial\", 'B', 16)\n",
        "    pdf.cell(0, 10, \"NETSYN: NIDS Post-Incident Forensic Report\", ln=True, align='C')\n",
        "\n",
        "    # Placeholder QR Code (updated later with hash)\n",
        "    qr_data = f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\nHash: [To be computed]\"\n",
        "    qr = qrcode.make(qr_data)\n",
        "    qr_img_path = \"report_qr.png\"\n",
        "    qr.save(qr_img_path)\n",
        "    try:\n",
        "        if os.path.exists(qr_img_path):\n",
        "            pdf.image(qr_img_path, x=page_width - 20, y=5, w=20)\n",
        "        else:\n",
        "            pdf.set_font(\"Courier\", size=10)\n",
        "            pdf.cell(0, 10, \"QR Code not available\", ln=True, align='R')\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Failed to embed initial QR code: {str(e)}\")\n",
        "        pdf.set_font(\"Courier\", size=10)\n",
        "        pdf.cell(0, 10, \"QR Code embedding failed\", ln=True, align='R')\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Horizontal Line Separator\n",
        "    pdf.set_line_width(0.5)\n",
        "    pdf.line(10, pdf.get_y(), page_width + 10, pdf.get_y())\n",
        "    pdf.ln(5)\n",
        "\n",
        "    # Risk Score Calculation\n",
        "    total_packets = len(df)\n",
        "    anomalies = df[df['Anomaly_Score_Iso'] == -1]\n",
        "    anomaly_rate = len(anomalies) / total_packets if total_packets > 0 else 0\n",
        "    critical_alerts = sum(1 for a in alerts if a.threat_level in [\"High\", \"Critical\"])\n",
        "    malware_count = len(malware_detected) + len([r for r in yara_results if \"No YARA matches\" not in r])\n",
        "    attack_count = len([a for a in attack_types if \"No specific attack\" not in a])\n",
        "    avg_entropy = np.mean([calculate_entropy(p) for p in payloads[:15]]) if payloads else 0\n",
        "    pkt_rate = total_packets / max((df['Timestamp'].max() - df['Timestamp'].min()).total_seconds(), 1) if total_packets > 1 else 0\n",
        "    unique_ips = df['Source_IP'].nunique()\n",
        "\n",
        "    risk_features = pd.DataFrame([{\n",
        "        'anomaly_rate': anomaly_rate, 'critical_alerts_per_1k': critical_alerts / (total_packets / 1000 + 1),\n",
        "        'malware_per_10_payloads': malware_count / (len(payloads) / 10 + 1), 'attack_count': attack_count,\n",
        "        'avg_entropy': avg_entropy, 'pkt_rate': pkt_rate, 'unique_ips': unique_ips / (total_packets / 1000 + 1)\n",
        "    }])\n",
        "    synthetic_data = pd.DataFrame({\n",
        "        'anomaly_rate': np.random.uniform(0, 0.5, 1000), 'critical_alerts_per_1k': np.random.uniform(0, 10, 1000),\n",
        "        'malware_per_10_payloads': np.random.uniform(0, 5, 1000), 'attack_count': np.random.randint(0, 10, 1000),\n",
        "        'avg_entropy': np.random.uniform(0, 8, 1000), 'pkt_rate': np.random.uniform(0, 1000, 1000),\n",
        "        'unique_ips': np.random.uniform(0, 10, 1000)\n",
        "    })\n",
        "    synthetic_data['risk'] = ((synthetic_data['anomaly_rate'] > 0.1).astype(int) * 20 +\n",
        "                             (synthetic_data['critical_alerts_per_1k'] > 2).astype(int) * 30 +\n",
        "                             (synthetic_data['malware_per_10_payloads'] > 1).astype(int) * 25 +\n",
        "                             (synthetic_data['attack_count'] > 3).astype(int) * 15 +\n",
        "                             (synthetic_data['avg_entropy'] > 6).astype(int) * 10).clip(0, 100)\n",
        "    lgb_risk = lgb.LGBMRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
        "    lgb_risk.fit(synthetic_data.drop('risk', axis=1), synthetic_data['risk'])\n",
        "    risk_score = round(min(max(lgb_risk.predict(risk_features)[0], 0), 100), 2)\n",
        "\n",
        "    # 1. Executive Summary\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(0, 10, \"1. Executive Summary\", ln=True, align='C')\n",
        "    pdf.set_font(\"Courier\", size=10)\n",
        "    table_width = page_width\n",
        "    col_widths = [40, table_width - 40]\n",
        "    pdf.cell(col_widths[0], 8, \"Metric\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, \"Value\", border=1, ln=True)\n",
        "    top_sender = Counter(df['Source_IP']).most_common(1)[0] if not df['Source_IP'].empty else (\"N/A\", 0)\n",
        "    top_receiver = Counter(df['Destination_IP']).most_common(1)[0] if not df['Destination_IP'].empty else (\"N/A\", 0)\n",
        "    rows = [\n",
        "        (\"Date\", datetime.now().strftime('%Y-%m-%d')),\n",
        "        (\"Packets Analyzed\", f\"{total_packets:,}\"),\n",
        "        (\"Anomalous Packets\", f\"{len(anomalies):,}\"),\n",
        "        (\"Risk Score\", f\"{risk_score}/100\"),\n",
        "        (\"Top Sender\", f\"{top_sender[0]} ({resolve_ip(top_sender[0])}): {top_sender[1]:,}\"),\n",
        "        (\"Top Receiver\", f\"{top_receiver[0]} ({resolve_ip(top_receiver[0])}): {top_receiver[1]:,}\")\n",
        "    ]\n",
        "    for label, value in rows:\n",
        "        pdf.cell(col_widths[0], 8, label, border=1)\n",
        "        pdf.cell(col_widths[1], 8, value, border=1, ln=True)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # 2. Security Alerts\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(0, 10, \"2. Security Alerts\", ln=True, align='C')\n",
        "    pdf.set_font(\"Courier\", size=8)\n",
        "    if not alerts:\n",
        "        pdf.cell(0, 6, \"No security alerts detected.\", ln=True)\n",
        "    else:\n",
        "        col_widths = [15, 30, 15, 15, 30, 75]\n",
        "        total_table_width = sum(col_widths)\n",
        "        if total_table_width > page_width:\n",
        "            scale = page_width / total_table_width\n",
        "            col_widths = [w * scale for w in col_widths]\n",
        "        headers = [\"SID\", \"Message\", \"Threat\", \"Conf.\", \"Timestamp\", \"Flow\"]\n",
        "        for i, header in enumerate(headers):\n",
        "            pdf.cell(col_widths[i], 8, header, border=1, align='C')\n",
        "        pdf.ln()\n",
        "        for alert in alerts[:15]:\n",
        "            flow_str = f\"{alert.src_ip}:{alert.src_port or 'N/A'} -> {alert.dst_ip}:{alert.dst_port or 'N/A'}\"\n",
        "            flow_display = flow_str if len(flow_str) <= 40 else flow_str[:37] + \"...\"\n",
        "            msg_str = alert.message[:20] + \"...\" if len(alert.message) > 20 else alert.message\n",
        "            pdf.cell(col_widths[0], 8, str(alert.sid), border=1, align='C')\n",
        "            pdf.cell(col_widths[1], 8, msg_str, border=1, align='L')\n",
        "            pdf.cell(col_widths[2], 8, alert.threat_level[:8], border=1, align='C')\n",
        "            pdf.cell(col_widths[3], 8, f\"{alert.confidence:.2f}\", border=1, align='C')\n",
        "            pdf.cell(col_widths[4], 8, alert.timestamp.strftime('%Y-%m-%d %H:%M'), border=1, align='C')\n",
        "            pdf.cell(col_widths[5], 8, flow_display, border=1, align='L')\n",
        "            pdf.ln()\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # 3. Network Traffic Overview\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(0, 10, \"3. Network Traffic Overview\", ln=True, align='C')\n",
        "    pdf.set_font(\"Courier\", size=10)\n",
        "    table_width = page_width\n",
        "    col_widths = [40, table_width - 40]\n",
        "    pdf.cell(col_widths[0], 8, \"Metric\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, \"Value\", border=1, ln=True)\n",
        "    pdf.cell(col_widths[0], 8, \"Unique Source IPs\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, str(df['Source_IP'].nunique()), border=1, ln=True)\n",
        "    pdf.cell(col_widths[0], 8, \"Unique Dest. IPs\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, str(df['Destination_IP'].nunique()), border=1, ln=True)\n",
        "    pdf.cell(col_widths[0], 8, \"Avg Packet Length\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, f\"{df['Packet_Length'].mean():.2f} bytes\", border=1, ln=True)\n",
        "    pdf.cell(col_widths[0], 8, \"ARP Packets\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, str(arp_count), border=1, ln=True)\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, \"Top Source IPs:\", ln=True)\n",
        "    col_widths = [40, 90, 20]\n",
        "    pdf.cell(col_widths[0], 8, \"IP Address\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, \"Geolocation\", border=1)\n",
        "    pdf.cell(col_widths[2], 8, \"Packets\", border=1, ln=True)\n",
        "    for ip, count in Counter(df['Source_IP']).most_common(5):\n",
        "        pdf.cell(col_widths[0], 8, ip, border=1)\n",
        "        pdf.cell(col_widths[1], 8, geoip_lookup(ip)[:40], border=1)\n",
        "        pdf.cell(col_widths[2], 8, str(count), border=1, ln=True)\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, \"Protocol Distribution:\", ln=True)\n",
        "    col_widths = [40, table_width - 40]\n",
        "    for proto, count in df['Protocol'].value_counts().items():\n",
        "        pdf.cell(col_widths[0], 8, proto, border=1)\n",
        "        pdf.cell(col_widths[1], 8, f\"{count} ({count/total_packets*100:.1f}%)\", border=1, ln=True)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # 4. Detected Attack Types\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(0, 10, \"4. Detected Attack Types\", ln=True, align='C')\n",
        "    pdf.set_font(\"Courier\", size=10)\n",
        "    pdf.cell(0, 6, \"Identified Attacks:\", ln=True)\n",
        "    for attack in attack_types[:10]:\n",
        "        pdf.cell(10, 6, \"- \", ln=False)\n",
        "        pdf.multi_cell(0, 6, attack[:80])\n",
        "    if not attack_types:\n",
        "        pdf.cell(0, 6, \"No specific attacks detected.\", ln=True)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # 5. Malware and Threat Intelligence\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(0, 10, \"5. Malware and Threat Intelligence\", ln=True, align='C')\n",
        "    pdf.set_font(\"Courier\", size=10)\n",
        "    pdf.cell(0, 6, \"Signature-Based Malware:\", ln=True)\n",
        "    for (name, sig), count in malware_detected[:5]:\n",
        "        pdf.cell(10, 6, \"- \", ln=False)\n",
        "        pdf.cell(0, 6, f\"{name}: {count} ({sig})\", ln=True)\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, \"ClamAV Results:\", ln=True)\n",
        "    for r in clamav_results[:5]:\n",
        "        pdf.cell(10, 6, \"- \", ln=False)\n",
        "        pdf.multi_cell(0, 6, r[:80])\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, \"YARA Results:\", ln=True)\n",
        "    for r in yara_results[:5]:\n",
        "        pdf.cell(10, 6, \"- \", ln=False)\n",
        "        pdf.multi_cell(0, 6, r[:80])\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, \"VirusTotal Results:\", ln=True)\n",
        "    for r in vt_results[:5]:\n",
        "        pdf.cell(10, 6, \"- \", ln=False)\n",
        "        pdf.multi_cell(0, 6, r[:80])\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, \"OTX Threat Intel (Top IPs):\", ln=True)\n",
        "    otx_results = [otx_lookup(ip) for ip, _ in Counter(df['Source_IP']).most_common(5)]\n",
        "    for ip, res in list(zip([x[0] for x in Counter(df['Source_IP']).most_common(5)], otx_results))[:5]:\n",
        "        pdf.cell(10, 6, \"- \", ln=False)\n",
        "        pdf.cell(0, 6, f\"{ip}: {res[:50]}\", ln=True)\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, \"AbuseIPDB Insights (Top IPs):\", ln=True)\n",
        "    for ip, res in list(zip([x[0] for x in Counter(df['Source_IP']).most_common(5)], abuseipdb_results))[:5]:\n",
        "        pdf.cell(10, 6, \"- \", ln=False)\n",
        "        pdf.cell(0, 6, f\"{ip}: {res[:50]}\", ln=True)\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, \"Top DNS Queries:\", ln=True)\n",
        "    for (query, src, dst), count in dns_queries[:5]:\n",
        "        pdf.cell(10, 6, \"- \", ln=False)\n",
        "        pdf.cell(0, 6, f\"{query[:30]} from {src} to {dst}: {count}\", ln=True)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # 6. Behavioral Insights\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(0, 10, \"6. Behavioral Insights\", ln=True, align='C')\n",
        "    pdf.set_font(\"Courier\", size=10)\n",
        "    suspicious_dbscan, suspicious_kmeans_ips = advanced_behavioral_profiling(df)\n",
        "    entropy_trend, high_entropy_spikes = network_entropy_analysis(payloads, df)\n",
        "    col_widths = [40, table_width - 40]\n",
        "    pdf.cell(col_widths[0], 8, \"Metric\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, \"Value\", border=1, ln=True)\n",
        "    pdf.cell(col_widths[0], 8, \"DBSCAN Outliers\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, f\"{len(suspicious_dbscan)} IPs\", border=1, ln=True)\n",
        "    pdf.cell(col_widths[0], 8, \"Top Outlier IPs\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, \", \".join(suspicious_dbscan[:5]), border=1, ln=True)\n",
        "    pdf.cell(col_widths[0], 8, \"KMeans High Pkt IPs\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, f\"{len(suspicious_kmeans_ips)} IPs\", border=1, ln=True)\n",
        "    pdf.cell(col_widths[0], 8, \"Top KMeans IPs\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, \", \".join(suspicious_kmeans_ips[:5]), border=1, ln=True)\n",
        "    pdf.cell(col_widths[0], 8, \"Entropy Spikes\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, f\"{high_entropy_spikes} (>6, 5-min intervals)\", border=1, ln=True)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # 7. Forensic Analysis\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(0, 10, \"7. Forensic Analysis\", ln=True, align='C')\n",
        "    pdf.set_font(\"Courier\", size=10)\n",
        "    pdf.cell(0, 6, f\"Carved Files: {len(carved_files)}\", ln=True)\n",
        "    col_widths = [30, 80, 30, 50]\n",
        "    pdf.cell(col_widths[0], 8, \"Type\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, \"Filename\", border=1)\n",
        "    pdf.cell(col_widths[2], 8, \"Size (bytes)\", border=1)\n",
        "    pdf.cell(col_widths[3], 8, \"YARA Matches\", border=1, ln=True)\n",
        "    for info in carved_files[:5]:\n",
        "        ftype, fname, size, *matches = info if len(info) > 3 else (info[0], info[1], info[2], [\"None\"])\n",
        "        pdf.cell(col_widths[0], 8, ftype, border=1)\n",
        "        pdf.cell(col_widths[1], 8, os.path.basename(fname)[:30], border=1)\n",
        "        pdf.cell(col_widths[2], 8, str(size), border=1)\n",
        "        pdf.cell(col_widths[3], 8, \", \".join(matches[0])[:20] if matches else \"None\", border=1, ln=True)\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, f\"TCP Sessions: {len(tcp_sessions)}\", ln=True)\n",
        "    col_widths = [60, 60, 30]\n",
        "    pdf.cell(col_widths[0], 8, \"Source\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, \"Destination\", border=1)\n",
        "    pdf.cell(col_widths[2], 8, \"Bytes\", border=1, ln=True)\n",
        "    for (src, sport, dst, dport), data in list(tcp_sessions.items())[:5]:\n",
        "        pdf.cell(col_widths[0], 8, f\"{src}:{sport}\"[:20], border=1)\n",
        "        pdf.cell(col_widths[1], 8, f\"{dst}:{dport}\"[:20], border=1)\n",
        "        pdf.cell(col_widths[2], 8, str(len(data)), border=1, ln=True)\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, \"HTTP Sessions:\", ln=True)\n",
        "    if not http_sessions:\n",
        "        pdf.cell(0, 6, \"No HTTP sessions detected.\", ln=True)\n",
        "    else:\n",
        "        col_widths = [20, 100, 50]\n",
        "        pdf.cell(col_widths[0], 8, \"Type\", border=1)\n",
        "        pdf.cell(col_widths[1], 8, \"Details\", border=1)\n",
        "        pdf.cell(col_widths[2], 8, \"Flow\", border=1, ln=True)\n",
        "        for s in http_sessions[:5]:\n",
        "            pdf.cell(col_widths[0], 8, s['Type'], border=1)\n",
        "            pdf.cell(col_widths[1], 8, f\"{s.get('Method', s.get('Status'))} {s.get('Host', '')}{s.get('Path', '')}\"[:50], border=1)\n",
        "            pdf.cell(col_widths[2], 8, f\"{s['Src_IP']} -> {s['Dst_IP']}\"[:25], border=1, ln=True)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # 8. Advanced NIDS Features\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(0, 10, \"8. Advanced NIDS Features\", ln=True, align='C')\n",
        "    pdf.set_font(\"Courier\", size=10)\n",
        "    pdf.cell(0, 6, \"Protocol Anomalies:\", ln=True)\n",
        "    col_widths = [40, table_width - 40]\n",
        "    pdf.cell(col_widths[0], 8, \"Unusual TCP Flags\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, f\"{len(unusual_flags)} detected: {', '.join([f'{f[0]} ({f[1]})' for f in unusual_flags[:3]])}\"[:50], border=1, ln=True)\n",
        "    pdf.cell(col_widths[0], 8, \"Malformed Packets\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, str(malformed_packets), border=1, ln=True)\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, \"Session Hijacking:\", ln=True)\n",
        "    pdf.cell(col_widths[0], 8, \"Anomalies\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, f\"{len(seq_anomalies)} detected: {', '.join([f'{s[0]}:{s[2]}' for s in seq_anomalies[:3]])}\"[:50], border=1, ln=True)\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, \"Covert Channels:\", ln=True)\n",
        "    pdf.cell(col_widths[0], 8, \"DNS Tunneling\", border=1)\n",
        "    pdf.cell(col_widths[1], 8, f\"{len(covert_channels)} detected: {', '.join([c[0][:20] for c in covert_channels[:3]])}\"[:50], border=1, ln=True)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # 9. Visual Analytics\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(0, 10, \"9. Visual Analytics\", ln=True, align='C')\n",
        "    pdf.set_font(\"Courier\", size=10)\n",
        "    pdf.cell(0, 6, \"Attached Visualizations:\", ln=True)\n",
        "    visual_files = [\"length_distribution.png\", \"protocol_distribution.png\", \"anomaly_timeline.png\", \"threat_heatmap.png\", \"pca_visual.png\"]\n",
        "    for img in visual_files:\n",
        "        try:\n",
        "            if os.path.exists(img) and os.path.getsize(img) > 0:\n",
        "                pdf.image(img, x=10, w=180)\n",
        "            else:\n",
        "                pdf.cell(0, 6, f\"Warning: {img} not found or empty\", ln=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to embed {img}: {str(e)}\")\n",
        "            pdf.cell(0, 6, f\"Warning: Failed to embed {img}\", ln=True)\n",
        "        pdf.ln(5)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # 10. Conclusion and Recommendations\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(0, 10, \"10. Conclusion and Recommendations\", ln=True, align='C')\n",
        "    pdf.set_font(\"Courier\", size=10)\n",
        "    pdf.cell(0, 6, \"Summary:\", ln=True)\n",
        "    pdf.multi_cell(0, 6, f\"Analysis of {len(df)} packets identified {len(anomalies)} anomalies and a risk score of {risk_score}/100.\\n\"\n",
        "                         f\"- Alerts: {len(alerts)} detected, {critical_alerts} high/critical.\\n\"\n",
        "                         f\"- Attacks: {len(attack_types)} observed, e.g., {attack_types[0] if attack_types else 'none'}.\\n\"\n",
        "                         f\"- Malware: {len(malware_detected)} signatures, {len([r for r in yara_results if 'No YARA matches' not in r])} YARA hits.\\n\"\n",
        "                         f\"- Behavioral: {len(suspicious_dbscan)} DBSCAN outliers, {high_entropy_spikes} entropy spikes.\")\n",
        "    pdf.ln(5)\n",
        "    pdf.cell(0, 6, \"Recommendations:\", ln=True)\n",
        "    pdf.multi_cell(0, 6, f\"- Immediate: {'Block IPs ' + ', '.join(high_rate_ips[:3]) + ' and investigate critical alerts.' if risk_score > 75 else 'Review high-rate IPs and alerts.'}\\n\"\n",
        "                         f\"- Post-Incident: Correlate AbuseIPDB data with timeline, preserve carved files for evidence, update policies based on abuse history and behavioral insights.\")\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Generate PDF Hash and Update QR Code\n",
        "    pdf_output = pdf.output(dest='S').encode('latin1')  # Get PDF content as bytes\n",
        "    pdf_hash = hashlib.sha256(pdf_output).hexdigest()[:16]  # Simple hash (first 16 chars for brevity)\n",
        "    qr_data = f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\nHash: {pdf_hash}\"\n",
        "    qr = qrcode.make(qr_data)\n",
        "    qr.save(qr_img_path)\n",
        "    try:\n",
        "        pdf.image(qr_img_path, x=page_width - 20, y=5, w=20)  # Replace initial QR code\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Failed to update QR code with hash: {str(e)}\")\n",
        "\n",
        "    # Output\n",
        "    pdf_output_path = \"NETSYN_NIDS_Post_Incident_Report.pdf\"\n",
        "    try:\n",
        "        pdf.output(pdf_output_path)\n",
        "        if os.path.exists(pdf_output_path) and os.path.getsize(pdf_output_path) > 0:\n",
        "            files.download(pdf_output_path)\n",
        "            print(f\"Report generated with hash: {pdf_hash}\")\n",
        "        else:\n",
        "            print(\"Error: PDF file was not generated or is empty.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating report: {str(e)}\")\n",
        "        # Fallback: Generate a minimal error PDF\n",
        "        error_pdf = FPDF()\n",
        "        error_pdf.add_page()\n",
        "        error_pdf.set_font(\"Courier\", size=10)\n",
        "        error_pdf.cell(0, 10, \"NETSYN: NIDS Post-Incident Forensic Report\", ln=True, align='C')\n",
        "        error_pdf.cell(0, 10, f\"Error: {str(e)}\", ln=True, align='C')\n",
        "        error_pdf.output(pdf_output_path)\n",
        "        if os.path.exists(pdf_output_path):\n",
        "            files.download(pdf_output_path)\n",
        "        else:\n",
        "            print(\"Error: Failed to generate even the error report PDF.\")\n",
        "\n",
        "def main():\n",
        "    # Prompt for URL or PCAP\n",
        "    choice = input(\"Would you like to scan a URL or upload a PCAP file? (Enter 'URL' or 'PCAP'): \").strip().upper()\n",
        "\n",
        "    if choice == 'URL':\n",
        "        url = input(\"Please enter the URL to scan with Nikto: \").strip()\n",
        "        nikto_scan(url)\n",
        "    elif choice == 'PCAP':\n",
        "        pcap_file = upload_pcap()\n",
        "        df, packets, payloads, arp_count, alerts, tcp_states, flow_stats, timings, ip_pairs = extract_features(pcap_file)\n",
        "        sid_counter = 1000001\n",
        "        unusual_flags, malformed_packets, alerts, sid_counter = detect_protocol_anomalies(packets, alerts, sid_counter)\n",
        "        df = detect_anomalies(df)\n",
        "        entropy_info = (calculate_entropy(payloads[0]) if payloads else 0, \"N/A\" if not payloads else \"High\" if calculate_entropy(payloads[0]) > 6 else \"Low\")\n",
        "        high_rate_ips = [ip for ip, count in Counter(df['Source_IP']).most_common(5) if count > 1500] if not df.empty else []\n",
        "        attack_types = detect_attack_types(df, arp_count, alerts, tcp_states, flow_stats, ip_pairs)\n",
        "        malware_detected = detect_malware(payloads)\n",
        "        clamav_results = clamav_scan(payloads)\n",
        "        yara_results = yara_scan(payloads)\n",
        "        payload_hashes = hash_payloads(payloads)\n",
        "        vt_results = virustotal_lookup(payload_hashes)\n",
        "        dns_queries = extract_dns_queries(packets)\n",
        "        abuseipdb_results = [abuseipdb_lookup(ip) for ip in [ip for ip, _ in Counter(df['Source_IP']).most_common(5)] if not df.empty] or [\"N/A\"]\n",
        "        seq_anomalies, alerts, sid_counter = detect_session_hijacking(packets, tcp_states, alerts, sid_counter)\n",
        "        covert_channels, alerts, sid_counter = detect_covert_channels(packets, payloads, alerts, sid_counter)\n",
        "\n",
        "        for i, p in enumerate(payloads[:15]):\n",
        "            entropy = calculate_entropy(p)\n",
        "            if i < len(alerts):\n",
        "                alerts[i].entropy = entropy\n",
        "                alerts[i].packet_hash = hashlib.sha256(p).hexdigest()[:16]\n",
        "\n",
        "        alerts = ml_threat_scoring(df, alerts, entropy_info)\n",
        "        generate_advanced_visuals(df)\n",
        "        carved_files = carve_files_from_payloads(payloads)\n",
        "        tcp_sessions = reconstruct_tcp_sessions(packets)\n",
        "        http_sessions, suspicious_protocols = deep_packet_inspection(packets)\n",
        "        memory_artifacts = simulate_memory_artifacts(payloads)\n",
        "        timeline_events = forensic_timeline_analysis(df, alerts)\n",
        "\n",
        "        generate_report(df, packets, payloads, attack_types, malware_detected, clamav_results, yara_results, vt_results, dns_queries,\n",
        "                        high_rate_ips, arp_count, alerts, entropy_info, tcp_states, flow_stats, timings, ip_pairs,\n",
        "                        carved_files, tcp_sessions, http_sessions, suspicious_protocols, memory_artifacts, timeline_events,\n",
        "                        abuseipdb_results, unusual_flags, malformed_packets, seq_anomalies, covert_channels)\n",
        "        print(\"Report downloaded as 'NETSYN_NIDS_Post_Incident_Report.pdf'\")\n",
        "    else:\n",
        "        print(\"Invalid choice. Please enter 'URL' or 'PCAP'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
